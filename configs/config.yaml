defaults:
  - _self_
  - pipeline: pipeline

mlflow:
  experiment_name: Retrieval
  run_name: BAAI/bge-small-en-v1.5
  description: test run
  tags:
    experiment: Retrieval
    dataset: QUAC
    model: BAAI/bge-small-en-v1.5
    # additional tags will be added from the config below


datasets: ./datasets/
embeddings: ./artifacts/embeddings/
experiments: ./artifacts/experiments/
colbert_index: ./artifacts/colbert_index

preprocess:
  dataset: SQUAD #QUAC / HotPotQA
  n_contexts: # 1000 # for testing
  concat_train_validation: True

retrieval:
  # comment all the below chunker until the model if the pipeline is used
  chunker:
#    name: SentenceSplitter
#    params:
#      chunk_size: 128
#      chunk_overlap: 16
#    name: SemanticSplitterNodeParser
#    params:
#      #embed_model: BAAI/bge-small-en-v1.5
#      breakpoint_percentile_threshold: 95
#      buffer_size: 1
#    name: TokenTextSplitter
#    params:
#      chunk_size: 256
#      chunk_overlap: 30
#      separator: " "
#    name: RecursiveCharacterTextSplitter
#    params:
#      separators: # default
#        - "\n\n"
#        - "\n"
#        - " "
#        - ""
#      name: SentenceWindowNodeParser
#      params:
#        window_size: 1

  model:
    model_name: BAAI/bge-small-en-v1.5
    embed_batch_size: 32
    normalize: true

  reranking: false
  reranker:
    model_name: cross-encoder/ms-marco-TinyBERT-L-2-v2
    predict:
      batch_size: 32
      show_progress_bar: true
      #num_workers: 6 DON'T use it, it is buggy, same scores are returned!

  evaluation:
    top_ks:
      - 1
      - 3
      - 5
      - 7
      - 10

colbert_retrieval:
  model_name: colbert-ir/colbertv2.0
  max_document_length: 128
  question_batch_size: 5000
  evaluation:
    top_ks:
      - 1
      - 3
      - 5
      - 7
      - 10

# change default log directory for hydra
hydra:
  run:
    dir: ./artifacts/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}
